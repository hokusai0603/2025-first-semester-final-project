"""
AI é‡‘èäº¤æ˜“ç³»çµ± - é‡å° 2020-2024 å°è‚¡å¸‚å ´
ç›®æ¨™ï¼šä½¿ç”¨æ©Ÿå™¨å­¸ç¿’æ¨¡å‹é æ¸¬è‚¡ç¥¨èµ°å‹¢ï¼Œä¸¦èˆ‡ 0050 é€²è¡Œæ¯”è¼ƒ

é¸å®šæ¨™çš„ï¼š
1. 2330.TW å°ç©é›» (æ¬Šå€¼å‹/å¤§ç›¤é€£å‹•é«˜)
2. 2317.TW é´»æµ· (AIé¡Œæ/è¶¨å‹¢å‹)
3. 2603.TW é•·æ¦® (æ™¯æ°£å¾ªç’°/é«˜æ³¢å‹•å‹)
å°æ¨™ï¼š0050.TW å…ƒå¤§å°ç£50

"""

import yfinance as yf
import pandas as pd
import numpy as np
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
from sklearn.preprocessing import StandardScaler
import joblib
import warnings
from datetime import datetime, timedelta
import matplotlib.pyplot as plt
import os
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim

warnings.filterwarnings('ignore')

# ==================== é…ç½®é¡ï¼šé›†ä¸­ç®¡ç†å¯èª¿åƒæ•¸ ====================

class TradingConfig:
    """
    äº¤æ˜“ç³»çµ±é…ç½®é¡ - é›†ä¸­ç®¡ç†æ‰€æœ‰å¯èª¿åƒæ•¸
    
    èª¿æ•´å»ºè­°ï¼š
    1. æ¨¡å‹æ›´å¼·å¤§ä½†è¨“ç·´æ…¢ï¼šå¢å¤§ hidden_dims, embedding_dim, epochs
    2. é˜²æ­¢éæ“¬åˆï¼šå¢åŠ  dropout, æ¸›å°‘ embedding_dim
    3. æ›´ä¿å®ˆäº¤æ˜“ï¼šæé«˜ buy_threshold, é™ä½ sell_threshold
    4. æ›´æ¿€é€²äº¤æ˜“ï¼šé™ä½ buy_threshold, æé«˜ sell_threshold
    """
    
    # æ¨¡å‹æ¶æ§‹åƒæ•¸
    EMBEDDING_DIM = 20  # 16->20 æå‡è¡¨å¾µèƒ½åŠ›
    HIDDEN_DIMS = (128, 64, 32)  # å¢åŠ å±¤æ•¸å’Œå¯¬åº¦
    DROPOUT = 0.15  # 0.1->0.15 é˜²æ­¢éæ“¬åˆ
    
    # è¨“ç·´åƒæ•¸
    LEARNING_RATE = 1e-3
    EPOCHS = 50  # 40->50 æ›´å……åˆ†è¨“ç·´
    EPISODES_PER_EPOCH = 80  # 60->80 æ›´å¤š episode
    K_SHOT = 5  # Few-shot æ”¯æŒæ¨£æœ¬æ•¸
    Q_QUERY = 12  # 10->12 æŸ¥è©¢æ¨£æœ¬æ•¸
    TEMPERATURE = 0.8  # 1.0->0.8 æ›´éŠ³åˆ©çš„æ±ºç­–é‚Šç•Œ
    
    # äº¤æ˜“ç­–ç•¥åƒæ•¸
    BUY_THRESHOLD = 0.005  # 0.004->0.005 æ›´ä¿å®ˆï¼ˆ0.5%ï¼‰
    HOLD_THRESHOLD = 0.002  # æŒæœ‰å€é–“
    SELL_THRESHOLD = -0.003  # æ–°å¢æ˜ç¢ºè³£å‡ºé–€æª»
    
    # ç‰¹å¾µå·¥ç¨‹åƒæ•¸
    RSI_PERIOD = 14
    SMA_PERIOD = 20
    ATR_PERIOD = 14
    LOG_RETURN_PERIODS = [5, 10, 20, 30]  # å¢åŠ 30æ—¥æ”¶ç›Šç‡
    
    # å›æ¸¬åƒæ•¸
    INITIAL_CAPITAL = 1000000
    COMMISSION_RATE = 0.002  # 0.2% å–®é‚Šæ‰‹çºŒè²»
    
    # æ±ºç­–è§£é‡‹åƒæ•¸
    ENABLE_EXPLANATION = True  # æ˜¯å¦è¼¸å‡ºæ±ºç­–è§£é‡‹
    TOP_FEATURES_TO_SHOW = 5  # é¡¯ç¤ºå‰Nå€‹é‡è¦ç‰¹å¾µ

# ==================== ç¬¬ä¸€éƒ¨åˆ†ï¼šæŠ€è¡“æŒ‡æ¨™è¨ˆç®— ====================

# ==================== æ–°å¢ï¼šåº¦é‡å­¸ç¿’ï¼ˆåŸå‹ç¶²è·¯ï¼‰æ¨¡å‹ ====================

class ProtoEncoder(nn.Module):
    def __init__(self, input_dim: int, embedding_dim: int = 16, hidden_dims=(64, 32), dropout: float = 0.1):
        super().__init__()
        layers = []
        last = input_dim
        for h in hidden_dims:
            layers += [nn.Linear(last, h), nn.ReLU()]
            if dropout > 0:
                layers += [nn.Dropout(dropout)]
            last = h
        layers += [nn.Linear(last, embedding_dim)]
        self.net = nn.Sequential(*layers)

    def forward(self, x):
        return self.net(x)


class ProtoNet:
    """ç°¡åŒ–ç‰ˆ Prototypical Networks for tabular classification.
    - ä½¿ç”¨ MLP å–å¾— embedding
    - episodic è¨“ç·´ï¼šæ¯å›åˆå°æ¯ä¸€é¡æŠ½ K æ”¯æŒèˆ‡ Q æŸ¥è©¢ï¼Œæœ€å°åŒ–æŸ¥è©¢åˆ°å„åŸå‹è·é›¢çš„äº¤å‰ç†µ
    - æ¨è«–ï¼šä»¥å…¨è¨“ç·´é›†åµŒå…¥çš„é¡åˆ¥å¹³å‡å‘é‡ä½œç‚ºåŸå‹ï¼Œå–æœ€è¿‘åŸå‹
    
    å¯èª¿æ•´åƒæ•¸å»ºè­°ï¼š
    - embedding_dim: 8-32 (è¶Šå¤§è¶Šèƒ½æ•æ‰è¤‡é›œæ¨¡å¼ï¼Œä½†æ˜“éæ“¬åˆ)
    - hidden_dims: (32,16) å¿«é€Ÿ, (128,64) å¼·å¤§ä½†æ…¢
    - dropout: 0.0-0.3 (é˜²æ­¢éæ“¬åˆ)
    - lr: 1e-4 ä¿å®ˆ, 1e-3 æ¨™æº–, 1e-2 æ¿€é€²
    - epochs: 20-80 (æ›´å¤šè¼ªæ•¸å¯èƒ½æå‡ä½†è¨“ç·´æ™‚é–“é•·)
    - temperature: 0.5-2.0 (å½±éŸ¿æ±ºç­–é‚Šç•Œçš„å¹³æ»‘åº¦)
    """

    def __init__(self,
                 input_dim: int,
                 n_classes: int,
                 embedding_dim: int = 16,
                 hidden_dims=(64, 32),
                 dropout: float = 0.1,
                 lr: float = 1e-3,
                 epochs: int = 40,
                 episodes_per_epoch: int = 60,
                 K: int = 5,
                 Q: int = 10,
                 temperature: float = 1.0,
                 device: str = 'cpu'):
        self.input_dim = input_dim
        self.n_classes = n_classes
        self.embedding_dim = embedding_dim
        self.hidden_dims = hidden_dims
        self.dropout = dropout
        self.lr = lr
        self.epochs = epochs
        self.episodes_per_epoch = episodes_per_epoch
        self.K = K
        self.Q = Q
        self.temperature = temperature
        self.device = torch.device(device)

        self.encoder = ProtoEncoder(input_dim, embedding_dim, hidden_dims, dropout).to(self.device)
        self.scaler = StandardScaler()
        self.class_to_index = None
        self.index_to_class = None
        self.prototypes = None  # shape: (n_classes, embedding_dim)

    def _to_tensor(self, x_np):
        return torch.tensor(x_np, dtype=torch.float32, device=self.device)

    def _compute_prototypes(self, emb: torch.Tensor, y_idx: np.ndarray) -> torch.Tensor:
        # emb: (N, D), y_idx: (N,)
        protos = []
        for c in range(self.n_classes):
            m = (y_idx == c)
            if m.sum() == 0:
                # è‹¥æŸé¡åˆ¥æ²’æœ‰æ¨£æœ¬ï¼Œè¨­ç‚ºé›¶å‘é‡
                protos.append(torch.zeros(emb.shape[1], device=self.device))
            else:
                protos.append(emb[m].mean(dim=0))
        return torch.stack(protos, dim=0)  # (C, D)

    def fit(self, X: np.ndarray, y: np.ndarray):
        # å»ºç«‹æ¨™ç±¤ç´¢å¼•æ˜ å°„
        classes = np.sort(np.unique(y))
        self.class_to_index = {c: i for i, c in enumerate(classes)}
        self.index_to_class = {i: c for c, i in self.class_to_index.items()}
        y_idx = np.vectorize(self.class_to_index.get)(y)

        # æ¨™æº–åŒ–ç‰¹å¾µ
        X_scaled = self.scaler.fit_transform(X)
        X_tensor = self._to_tensor(X_scaled)

        optimizer = optim.Adam(self.encoder.parameters(), lr=self.lr)

        rng = np.random.default_rng(42)
        # ç‚ºæ¯ä¸€é¡å»ºç«‹ç´¢å¼•é›†åˆ
        idx_by_class = {c: np.where(y_idx == c)[0] for c in range(self.n_classes)}

        # è¨“ç·´ï¼ˆç°¡åŒ– episodicï¼‰
        self.encoder.train()
        for epoch in range(self.epochs):
            total_loss = 0.0
            episodes = 0
            for _ in range(self.episodes_per_epoch):
                support_idx = []
                query_idx = []
                valid = True
                for c in range(self.n_classes):
                    idxs = idx_by_class[c]
                    if len(idxs) == 0:
                        valid = False
                        break
                    # è‹¥æ•¸é‡ä¸è¶³ï¼Œå…è¨±é‡è¤‡æŠ½æ¨£
                    s = rng.choice(idxs, size=self.K, replace=(len(idxs) < self.K))
                    q = rng.choice(idxs, size=self.Q, replace=(len(idxs) < self.Q))
                    support_idx.append(s)
                    query_idx.append(q)
                if not valid:
                    continue

                support_idx = np.concatenate(support_idx)
                query_idx = np.concatenate(query_idx)

                X_sup = X_tensor[support_idx]
                X_que = X_tensor[query_idx]
                y_que = y_idx[query_idx]

                # å–å¾—åµŒå…¥èˆ‡åŸå‹
                z_sup = self.encoder(X_sup)  # (C*K, D)
                z_que = self.encoder(X_que)  # (C*Q, D)

                # è¨ˆç®—æ¯é¡çš„åŸå‹ï¼ˆsupport å¹³å‡ï¼‰
                # å°‡ support æ‹†å›å„é¡åˆ¥åˆ‡å¡Š
                z_chunks = torch.chunk(z_sup, self.n_classes, dim=0)
                protos = torch.stack([zc.mean(dim=0) for zc in z_chunks], dim=0)  # (C, D)

                # è·é›¢ -> logits
                # z_que: (Nq, D), protos: (C, D)
                # pairwise distances
                dists = torch.cdist(z_que, protos, p=2)  # (Nq, C)
                logits = - (dists ** 2) / self.temperature
                loss = F.cross_entropy(logits, torch.tensor(y_que, device=self.device))

                optimizer.zero_grad()
                loss.backward()
                optimizer.step()

                total_loss += loss.item()
                episodes += 1

            if episodes > 0 and (epoch + 1) % 5 == 0:
                avg_loss = total_loss / episodes
                print(f"ProtoNet è¨“ç·´ Epoch {epoch+1}/{self.epochs} - å¹³å‡æå¤±: {avg_loss:.4f}")

        # ä»¥æ‰€æœ‰è¨“ç·´æ¨£æœ¬å»ºç«‹æœ€çµ‚åŸå‹
        self.encoder.eval()
        with torch.no_grad():
            z_all = self.encoder(X_tensor)
            protos = self._compute_prototypes(z_all, y_idx)
        self.prototypes = protos.detach().cpu().numpy()
        return self

    def _embed(self, X: np.ndarray) -> np.ndarray:
        self.encoder.eval()
        X_scaled = self.scaler.transform(X)
        with torch.no_grad():
            z = self.encoder(self._to_tensor(X_scaled))
        return z.detach().cpu().numpy()

    def predict(self, X: np.ndarray) -> np.ndarray:
        Z = self._embed(X)  # (N, D)
        # è·é›¢åˆ°åŸå‹
        dists = ((Z[:, None, :] - self.prototypes[None, :, :]) ** 2).sum(axis=2)  # (N, C)
        idx = dists.argmin(axis=1)
        # æ˜ å›åŸå§‹æ¨™ç±¤
        return np.vectorize(self.index_to_class.get)(idx)

    def predict_proba(self, X: np.ndarray) -> np.ndarray:
        if self.prototypes is None or self.class_to_index is None:
            raise RuntimeError("ProtoNet å°šæœªå®Œæˆè¨“ç·´æˆ–åŸå‹æœªå»ºç«‹ï¼Œè«‹å…ˆå‘¼å« fit() å¾Œå†é€²è¡Œé æ¸¬æ©Ÿç‡")
        Z = self._embed(X)
        dists = ((Z[:, None, :] - self.prototypes[None, :, :]) ** 2).sum(axis=2)
        logits = - dists / self.temperature
        # softmax
        e = np.exp(logits - logits.max(axis=1, keepdims=True))
        p = e / e.sum(axis=1, keepdims=True)
        # éœ€å°æ‡‰åŸå§‹é¡åˆ¥é †åºï¼ˆ0,1,2ï¼‰
        # self.index_to_class: idx -> class_label
        order = [self.class_to_index[c] for c in sorted(self.class_to_index.keys())]
        return p[:, order]
    
    def explain_prediction(self, X: np.ndarray, feature_names=None) -> dict:
        """
        è§£é‡‹é æ¸¬æ±ºç­–ï¼šæä¾›ç‰¹å¾µè²¢ç»åº¦å’ŒåŸå‹è·é›¢åˆ†æ
        
        Args:
            X: è¼¸å…¥ç‰¹å¾µ (å–®ç­†æˆ–å¤šç­†)
            feature_names: ç‰¹å¾µåç¨±åˆ—è¡¨
        
        Returns:
            dict: åŒ…å«é æ¸¬ã€è·é›¢ã€ç‰¹å¾µè²¢ç»ç­‰è³‡è¨Š
        """
        if self.prototypes is None:
            raise RuntimeError("æ¨¡å‹å°šæœªè¨“ç·´")
        
        # æ”¯æ´ DataFrame è¼¸å…¥
        if isinstance(X, pd.DataFrame):
            if feature_names is None:
                feature_names = X.columns.tolist()
            X = X.values
        
        if feature_names is None:
            feature_names = [f"Feature_{i}" for i in range(X.shape[1])]
        
        # å–å¾—åµŒå…¥å’Œè·é›¢
        Z = self._embed(X)
        dists = ((Z[:, None, :] - self.prototypes[None, :, :]) ** 2).sum(axis=2)
        
        # é æ¸¬çµæœ
        predictions = dists.argmin(axis=1)
        probabilities = self.predict_proba(X)
        
        # ç‰¹å¾µé‡è¦æ€§åˆ†æï¼šè¨ˆç®—æ¯å€‹ç‰¹å¾µå°æœ€çµ‚æ±ºç­–çš„å½±éŸ¿
        # ä½¿ç”¨æ¢¯åº¦è¿‘ä¼¼æˆ–æ•æ„Ÿåº¦åˆ†æ
        X_scaled = self.scaler.transform(X)
        
        explanations = []
        for i in range(len(X)):
            pred_class = predictions[i]
            
            # è¨ˆç®—åˆ°å„åŸå‹çš„è·é›¢
            distances_to_protos = {
                f"åˆ°é¡åˆ¥{c}({['è³£å‡º/è§€æœ›', 'æŒæœ‰', 'è²·å…¥'][c]})è·é›¢": float(dists[i, c])
                for c in range(self.n_classes)
            }
            
            # ç‰¹å¾µæ•æ„Ÿåº¦ï¼šå°æ¯å€‹ç‰¹å¾µå¾®æ“¾å¾Œè§€å¯Ÿè·é›¢è®ŠåŒ–
            feature_importance = {}
            epsilon = 0.01
            
            for j, fname in enumerate(feature_names):
                X_perturbed = X_scaled[i:i+1].copy()
                X_perturbed[0, j] += epsilon
                
                # è¨ˆç®—å¾®æ“¾å¾Œåˆ°é æ¸¬é¡åˆ¥çš„è·é›¢è®ŠåŒ–
                with torch.no_grad():
                    Z_perturbed = self.encoder(self._to_tensor(X_perturbed)).cpu().numpy()
                dist_original = dists[i, pred_class]
                dist_perturbed = ((Z_perturbed - self.prototypes[pred_class:pred_class+1]) ** 2).sum()
                
                # è·é›¢è®ŠåŒ–é‡ï¼ˆæ­£å€¼è¡¨ç¤ºè©²ç‰¹å¾µå¢å¤§æœƒä½¿é æ¸¬æ›´ä¸ç¢ºå®šï¼‰
                sensitivity = float(dist_perturbed - dist_original)
                feature_importance[fname] = {
                    'value': float(X[i, j]),
                    'scaled_value': float(X_scaled[i, j]),
                    'sensitivity': sensitivity
                }
            
            # æŒ‰æ•æ„Ÿåº¦æ’åº
            sorted_features = sorted(feature_importance.items(), 
                                   key=lambda x: abs(x[1]['sensitivity']), 
                                   reverse=True)
            
            explanation = {
                'prediction': int(predictions[i]),
                'prediction_label': ['è³£å‡º/è§€æœ›', 'æŒæœ‰', 'è²·å…¥'][int(predictions[i])],
                'probabilities': {
                    'è³£å‡º/è§€æœ›': float(probabilities[i, 0]),
                    'æŒæœ‰': float(probabilities[i, 1]),
                    'è²·å…¥': float(probabilities[i, 2])
                },
                'distances': distances_to_protos,
                'top_features': [
                    {
                        'name': name,
                        'value': info['value'],
                        'scaled_value': info['scaled_value'],
                        'sensitivity': info['sensitivity'],
                        'impact': 'æ¨å‹•æ±ºç­–' if info['sensitivity'] > 0 else 'æŠ‘åˆ¶æ±ºç­–'
                    }
                    for name, info in sorted_features[:5]
                ],
                'all_features': dict(sorted_features)
            }
            explanations.append(explanation)
        
        return explanations[0] if len(explanations) == 1 else explanations

    # ç”¨æ–¼æŒä¹…åŒ–
    def get_state(self):
        return {
            'model_type': 'prototypical',
            'input_dim': self.input_dim,
            'n_classes': self.n_classes,
            'embedding_dim': self.embedding_dim,
            'hidden_dims': self.hidden_dims,
            'dropout': self.dropout,
            'lr': self.lr,
            'epochs': self.epochs,
            'episodes_per_epoch': self.episodes_per_epoch,
            'K': self.K,
            'Q': self.Q,
            'temperature': self.temperature,
            'state_dict': {k: v.cpu().numpy() for k, v in self.encoder.state_dict().items()},
            'prototypes': self.prototypes,
            'scaler_mean_': self.scaler.mean_.copy(),
            'scaler_scale_': self.scaler.scale_.copy(),
            'class_to_index': self.class_to_index,
            'index_to_class': self.index_to_class,
        }

    @staticmethod
    def from_state(state: dict):
        model = ProtoNet(
            input_dim=state['input_dim'],
            n_classes=state['n_classes'],
            embedding_dim=state['embedding_dim'],
            hidden_dims=tuple(state['hidden_dims']),
            dropout=state['dropout'],
            lr=state['lr'],
            epochs=state['epochs'],
            episodes_per_epoch=state['episodes_per_epoch'],
            K=state['K'],
            Q=state['Q'],
            temperature=state['temperature'],
            device='cpu'
        )
        # é‚„åŸ encoder æ¬Šé‡
        sd = {k: torch.tensor(v) for k, v in state['state_dict'].items()}
        model.encoder.load_state_dict(sd)
        # é‚„åŸ scaler
        model.scaler.mean_ = np.array(state['scaler_mean_'])
        model.scaler.scale_ = np.array(state['scaler_scale_'])
        model.scaler.n_features_in_ = model.input_dim
        # é‚„åŸåŸå‹èˆ‡ç´¢å¼•æ˜ å°„
        model.prototypes = np.array(state['prototypes'])
        model.class_to_index = state['class_to_index']
        model.index_to_class = state['index_to_class']
        return model

def calculate_rsi(data, period=14):
    """
    è¨ˆç®—ç›¸å°å¼·å¼±æŒ‡æ¨™ (RSI)
    
    Args:
        data: DataFrameï¼Œå¿…é ˆåŒ…å« 'Close' æ¬„ä½
        period: RSI é€±æœŸï¼Œé è¨­14å¤©
    
    Returns:
        Series: RSI å€¼ (0-100)
    """
    delta = data['Close'].diff()
    gain = (delta.where(delta > 0, 0)).rolling(window=period).mean()
    loss = (-delta.where(delta < 0, 0)).rolling(window=period).mean()
    
    rs = gain / loss
    rsi = 100 - (100 / (1 + rs))
    return rsi


def calculate_sma(data, period=20):
    """
    è¨ˆç®—ç°¡å–®ç§»å‹•å¹³å‡ç·š (SMA)
    
    Args:
        data: DataFrameï¼Œå¿…é ˆåŒ…å« 'Close' æ¬„ä½
        period: SMA é€±æœŸï¼Œé è¨­20å¤©
    
    Returns:
        Series: SMA å€¼
    """
    return data['Close'].rolling(window=period).mean()


def calculate_atr(data, period=14):
    """
    è¨ˆç®—çœŸå¯¦æ³¢å‹•å¹…åº¦å‡å€¼ (ATR)
    
    Args:
        data: DataFrameï¼Œå¿…é ˆåŒ…å« 'High', 'Low', 'Close' æ¬„ä½
        period: ATR é€±æœŸï¼Œé è¨­14å¤©
    
    Returns:
        Series: ATR å€¼
    """
    high_low = data['High'] - data['Low']
    high_close = np.abs(data['High'] - data['Close'].shift())
    low_close = np.abs(data['Low'] - data['Close'].shift())
    
    true_range = pd.concat([high_low, high_close, low_close], axis=1).max(axis=1)
    atr = true_range.rolling(window=period).mean()
    return atr


def calculate_log_returns(data, periods=[5, 10, 20]):
    """
    è¨ˆç®—å°æ•¸æ”¶ç›Šç‡
    
    Args:
        data: DataFrameï¼Œå¿…é ˆåŒ…å« 'Close' æ¬„ä½
        periods: è¨ˆç®—é€±æœŸåˆ—è¡¨
    
    Returns:
        DataFrame: åŒ…å«å„é€±æœŸå°æ•¸æ”¶ç›Šç‡çš„æ¬„ä½
    """
    result = pd.DataFrame(index=data.index)
    for period in periods:
        result[f'LogReturn_{period}'] = np.log(data['Close'] / data['Close'].shift(period))
    return result


# ==================== ç¬¬äºŒéƒ¨åˆ†ï¼šç‰¹å¾µå·¥ç¨‹ ====================

def engineer_features(data):
    """
    å»ºç«‹å®Œæ•´çš„æŠ€è¡“ç‰¹å¾µ
    
    Args:
        data: åŸå§‹è‚¡åƒ¹ DataFrame
    
    Returns:
        DataFrame: åŒ…å«æ‰€æœ‰æŠ€è¡“ç‰¹å¾µçš„ DataFrame
    """
    df = data.copy()
    
    # 1. RSI (æ¨™æº–åŒ–è‡³ 0-1)
    df['RSI'] = calculate_rsi(df, period=14) / 100.0
    
    # 2. SMA èˆ‡ä¹–é›¢ç‡
    df['SMA_20'] = calculate_sma(df, period=20)
    df['SMA_Deviation'] = (df['Close'] - df['SMA_20']) / df['SMA_20']
    
    # 3. ATR (æ³¢å‹•ç‡æŒ‡æ¨™)
    df['ATR'] = calculate_atr(df, period=14)
    df['ATR_Normalized'] = df['ATR'] / df['Close']  # æ¨™æº–åŒ–
    
    # 4. å°æ•¸æ”¶ç›Šç‡
    log_returns = calculate_log_returns(df, periods=[5, 10, 20])
    df = pd.concat([df, log_returns], axis=1)
    
    # 5. æˆäº¤é‡è®ŠåŒ–ç‡
    df['Volume_Change'] = df['Volume'].pct_change(5)
    
    # 6. åƒ¹æ ¼å‹•èƒ½ (Momentum)
    df['Momentum_10'] = df['Close'] - df['Close'].shift(10)
    df['Momentum_20'] = df['Close'] - df['Close'].shift(20)
    
    # 7. è™•ç†ç„¡é™å€¼å’Œè¶…å¤§å€¼ï¼ˆæ›¿æ›ç‚º NaNï¼Œå¾ŒçºŒæœƒè¢« dropna ç§»é™¤ï¼‰
    df = df.replace([np.inf, -np.inf], np.nan)
    
    return df


def create_labels(data, threshold=0.004, hold_threshold=0.002):
    """
    å»ºç«‹é æ¸¬æ¨™ç±¤ï¼šé æ¸¬æ˜æ—¥çš„æ“ä½œç­–ç•¥ï¼ˆä¸‰åˆ†é¡ï¼‰
    è€ƒæ…®äº¤æ˜“æˆæœ¬ï¼Œè¨­å®šé–€æª»ç‚º 0.4% (æ‰‹çºŒè²»0.1425% + è­‰äº¤ç¨…0.3%)
    
    Args:
        data: DataFrame
        threshold: è²·å…¥é–€æª» (é è¨­ 0.4%)
        hold_threshold: æŒæœ‰é–€æª» (é è¨­ 0.2%)
    
    Returns:
        Series: æ¨™ç±¤ (0=è³£å‡º/è§€æœ›, 1=æŒæœ‰, 2=è²·å…¥)
    """
    future_close = data['Close'].shift(-1)
    future_open = data['Open'].shift(-1)
    
    # è¨ˆç®—é æœŸæ”¶ç›Šç‡
    expected_return = (future_close - future_open) / future_open
    
    # ä¸‰åˆ†é¡æ¨™ç±¤
    # 2: é æœŸæ”¶ç›Š > threshold (è²·å…¥)
    # 1: -hold_threshold <= é æœŸæ”¶ç›Š <= threshold (æŒæœ‰)
    # 0: é æœŸæ”¶ç›Š < -hold_threshold (è³£å‡º/è§€æœ›)
    labels = pd.Series(1, index=data.index)  # é è¨­ç‚ºæŒæœ‰
    labels[expected_return > threshold] = 2  # è²·å…¥
    labels[expected_return < -hold_threshold] = 0  # è³£å‡º/è§€æœ›
    
    return labels


# ==================== ç¬¬ä¸‰éƒ¨åˆ†ï¼šè³‡æ–™ç²å– ====================

def download_stock_data(stock_id, start_date='2015-01-01', end_date='2024-12-31'):
    """
    ä¸‹è¼‰è‚¡ç¥¨è³‡æ–™
    
    Args:
        stock_id: è‚¡ç¥¨ä»£ç¢¼ (ä¾‹å¦‚ '2330.TW')
        start_date: é–‹å§‹æ—¥æœŸ
        end_date: çµæŸæ—¥æœŸ
    
    Returns:
        DataFrame: è‚¡åƒ¹è³‡æ–™
    """
    print(f"æ­£åœ¨ä¸‹è¼‰ {stock_id} çš„è³‡æ–™...")
    data = yf.download(stock_id, start=start_date, end=end_date, progress=False)
    
    # è™•ç†å¤šå±¤ç´¢å¼•å•é¡Œï¼ˆç•¶åªä¸‹è¼‰ä¸€æ”¯è‚¡ç¥¨æ™‚ï¼Œyfinance å¯èƒ½è¿”å›å¤šå±¤ç´¢å¼•ï¼‰
    if isinstance(data.columns, pd.MultiIndex):
        data.columns = data.columns.droplevel(1)
    
    print(f"ä¸‹è¼‰å®Œæˆï¼å…± {len(data)} ç­†è³‡æ–™")
    return data


# ==================== ç¬¬å››éƒ¨åˆ†ï¼šæ¨¡å‹è¨“ç·´ ====================

class StockTradingModel:
    """è‚¡ç¥¨äº¤æ˜“é æ¸¬æ¨¡å‹é¡åˆ¥ï¼ˆå·²æ”¹ç‚ºåº¦é‡å­¸ç¿’ï¼šåŸå‹ç¶²è·¯ï¼‰"""
    
    def __init__(self, stock_id, feature_cols=None, use_few_shot=True):
        self.stock_id = stock_id
        self.use_few_shot = use_few_shot  # æ˜¯å¦ä½¿ç”¨ Few-Shot Learningï¼ˆé€é episodic è¨“ç·´é”æˆï¼‰
        self.model = None  # å°‡åœ¨ train() ä¸­å»ºç«‹ ProtoNet
        self.feature_cols = feature_cols or [
            'RSI', 'SMA_Deviation', 'ATR_Normalized',
            'LogReturn_5', 'LogReturn_10', 'LogReturn_20',
            'Volume_Change', 'Momentum_10', 'Momentum_20'
        ]
        self.few_shot_samples = {}
        
    def prepare_data(self, data, train_end='2023-12-31'):
        """
        æº–å‚™è¨“ç·´èˆ‡æ¸¬è©¦è³‡æ–™
        
        Args:
            data: å®Œæ•´è³‡æ–™é›†
            train_end: è¨“ç·´é›†çµæŸæ—¥æœŸ
        
        Returns:
            tuple: (X_train, X_test, y_train, y_test, train_data, test_data)
        """
        # å»ºç«‹ç‰¹å¾µ
        df = engineer_features(data)
        
        # å»ºç«‹æ¨™ç±¤ï¼ˆä¸‰åˆ†é¡ï¼š0=è³£å‡º/è§€æœ›, 1=æŒæœ‰, 2=è²·å…¥ï¼‰
        df['Target'] = create_labels(df, threshold=0.004, hold_threshold=0.002)
        
        # ç§»é™¤ NaN
        df = df.dropna()
        
        # åˆ‡åˆ†è¨“ç·´é›†èˆ‡æ¸¬è©¦é›†
        train_data = df.loc[:train_end]
        test_data = df.loc[train_end:]
        
        X_train = train_data[self.feature_cols]
        y_train = train_data['Target']
        X_test = test_data[self.feature_cols]
        y_test = test_data['Target']
        
        print(f"\nè¨“ç·´é›†ï¼š{len(train_data)} ç­† ({train_data.index[0].date()} ~ {train_data.index[-1].date()})")
        print(f"æ¸¬è©¦é›†ï¼š{len(test_data)} ç­† ({test_data.index[0].date()} ~ {test_data.index[-1].date()})")
        print(f"è¨“ç·´é›†æ­£æ¨£æœ¬æ¯”ä¾‹ï¼š{y_train.mean():.2%}")
        print(f"æ¸¬è©¦é›†æ­£æ¨£æœ¬æ¯”ä¾‹ï¼š{y_test.mean():.2%}")
        
        return X_train, X_test, y_train, y_test, train_data, test_data
    
    def calculate_sample_weights(self, y_train):
        """
        è¨ˆç®— Few-Shot Learning çš„æ¨£æœ¬æ¬Šé‡
        å°æ–¼å°‘æ•¸é¡åˆ¥çš„æ¨£æœ¬çµ¦äºˆæ›´é«˜çš„æ¬Šé‡
        
        Args:
            y_train: è¨“ç·´æ¨™ç±¤
        
        Returns:
            array: æ¨£æœ¬æ¬Šé‡
        """
        from sklearn.utils.class_weight import compute_sample_weight
        
        # è¨ˆç®—é¡åˆ¥åˆ†å¸ƒ
        class_counts = pd.Series(y_train).value_counts()
        print(f"\né¡åˆ¥åˆ†å¸ƒï¼š")
        for class_label in sorted(class_counts.index):
            count = class_counts[class_label]
            percentage = count / len(y_train) * 100
            class_name = ['è³£å‡º/è§€æœ›', 'æŒæœ‰', 'è²·å…¥'][int(class_label)]
            print(f"  {class_name} (é¡åˆ¥{int(class_label)}): {count:>4} ç­† ({percentage:>5.2f}%)")
        
        # ä½¿ç”¨ sklearn è¨ˆç®—å¹³è¡¡æ¬Šé‡
        sample_weights = compute_sample_weight('balanced', y_train)
        
        # Few-Shot å¢å¼·ï¼šå°æœ€å°‘æ•¸é¡åˆ¥é¡å¤–å¢åŠ æ¬Šé‡
        min_class = class_counts.idxmin()
        min_count = class_counts.min()
        max_count = class_counts.max()
        
        if min_count < max_count * 0.3:  # å¦‚æœæœ€å°‘é¡åˆ¥æ¨£æœ¬æ•¸ < æœ€å¤šé¡åˆ¥çš„30%
            boost_factor = 1.5  # é¡å¤–å¢å¼·ä¿‚æ•¸
            for i, label in enumerate(y_train):
                if label == min_class:
                    sample_weights[i] *= boost_factor
            print(f"\nå¥—ç”¨ Few-Shot Learningï¼šå° '{['è³£å‡º/è§€æœ›', 'æŒæœ‰', 'è²·å…¥'][int(min_class)]}' é¡åˆ¥æ¨£æœ¬æ¬Šé‡æå‡ {boost_factor}x")
        
        return sample_weights
    
    def augment_minority_samples(self, X_train, y_train, augment_ratio=0.5):
        """
        æ•¸æ“šå¢å¼·ï¼šç‚ºå°‘æ•¸é¡åˆ¥ç”Ÿæˆåˆæˆæ¨£æœ¬ (SMOTE-like)
        
        Args:
            X_train: è¨“ç·´ç‰¹å¾µ
            y_train: è¨“ç·´æ¨™ç±¤
            augment_ratio: å¢å¼·æ¯”ä¾‹
        
        Returns:
            tuple: (å¢å¼·å¾Œçš„X, å¢å¼·å¾Œçš„y)
        """
        from sklearn.utils import resample
        
        # è½‰æ›ç‚º numpy array ç¢ºä¿ä¸€è‡´æ€§
        if isinstance(X_train, pd.DataFrame):
            X_train_array = X_train.values
        else:
            X_train_array = np.array(X_train)
        
        if isinstance(y_train, pd.Series):
            y_train_array = y_train.values
        else:
            y_train_array = np.array(y_train)
        
        class_counts = pd.Series(y_train_array).value_counts()
        max_count = class_counts.max()
        
        X_augmented = [X_train_array]
        y_augmented = [y_train_array]
        
        # å°æ¯å€‹å°‘æ•¸é¡åˆ¥é€²è¡Œå¢å¼·
        for class_label in class_counts.index:
            count = class_counts[class_label]
            if count < max_count * 0.5:  # å¦‚æœæ¨£æœ¬æ•¸ < æœ€å¤šé¡åˆ¥çš„50%
                # è¨ˆç®—éœ€è¦å¢å¼·çš„æ¨£æœ¬æ•¸
                n_samples_needed = int((max_count * augment_ratio - count))
                if n_samples_needed > 0:
                    # å¾è©²é¡åˆ¥ä¸­é‡æ–°æ¡æ¨£ä¸¦æ·»åŠ å°å™ªè²
                    class_mask = y_train_array == class_label
                    X_class = X_train_array[class_mask]
                    
                    # é‡æ¡æ¨£
                    indices = np.random.choice(len(X_class), n_samples_needed, replace=True)
                    X_resampled = X_class[indices]
                    
                    # æ·»åŠ å°å™ªè²ï¼ˆ5%æ¨™æº–å·®ï¼‰
                    noise = np.random.normal(0, 0.05, X_resampled.shape)
                    X_std = np.std(X_resampled, axis=0)
                    X_resampled_noisy = X_resampled + noise * X_std
                    
                    y_resampled = np.array([class_label] * n_samples_needed)
                    
                    X_augmented.append(X_resampled_noisy)
                    y_augmented.append(y_resampled)
                    
                    class_name = ['è³£å‡º/è§€æœ›', 'æŒæœ‰', 'è²·å…¥'][int(class_label)]
                    print(f"  ç‚º '{class_name}' é¡åˆ¥ç”Ÿæˆ {n_samples_needed} å€‹å¢å¼·æ¨£æœ¬")
        
        # åˆä½µæ‰€æœ‰æ•¸æ“š
        X_final = np.vstack(X_augmented)
        y_final = np.concatenate(y_augmented)
        
        return X_final, y_final
    
    def train(self, X_train, y_train, config=None):
        """è¨“ç·´æ¨¡å‹ï¼ˆä½¿ç”¨ Prototypical Networkï¼‰"""
        if config is None:
            config = TradingConfig()
        
        print(f"\né–‹å§‹è¨“ç·´ {self.stock_id} çš„æ¨¡å‹...")
        print(f"ä½¿ç”¨é…ç½®ï¼šembedding_dim={config.EMBEDDING_DIM}, hidden={config.HIDDEN_DIMS}, "
              f"dropout={config.DROPOUT}, lr={config.LEARNING_RATE}, epochs={config.EPOCHS}")

        # å°å‡ºé¡åˆ¥åˆ†å¸ƒï¼ˆè§€å¯Ÿä¸å¹³è¡¡æƒ…æ³ï¼‰
        class_counts = pd.Series(y_train).value_counts()
        print("\né¡åˆ¥åˆ†å¸ƒï¼š")
        for class_label in sorted(class_counts.index):
            count = class_counts[class_label]
            percentage = count / len(y_train) * 100
            class_name = ['è³£å‡º/è§€æœ›', 'æŒæœ‰', 'è²·å…¥'][int(class_label)]
            print(f"  {class_name} (é¡åˆ¥{int(class_label)}): {count:>4} ç­† ({percentage:>5.2f}%)")

        X_np = X_train.values if isinstance(X_train, pd.DataFrame) else np.asarray(X_train)
        y_np = y_train.values if isinstance(y_train, (pd.Series, pd.DataFrame)) else np.asarray(y_train)

        # ä½¿ç”¨é…ç½®åƒæ•¸
        proto = ProtoNet(
            input_dim=X_np.shape[1], n_classes=3,
            embedding_dim=config.EMBEDDING_DIM,
            hidden_dims=config.HIDDEN_DIMS,
            dropout=config.DROPOUT,
            lr=config.LEARNING_RATE,
            epochs=config.EPOCHS,
            episodes_per_epoch=config.EPISODES_PER_EPOCH,
            K=config.K_SHOT,
            Q=config.Q_QUERY,
            temperature=config.TEMPERATURE,
            device='cpu'
        )
        proto.fit(X_np, y_np)
        self.model = proto
        return self.model
    
    def evaluate(self, X_test, y_test, enable_explanation=True):
        """è©•ä¼°æ¨¡å‹ï¼ˆå«æ±ºç­–è§£é‡‹ï¼‰"""
        X_np = X_test.values if isinstance(X_test, pd.DataFrame) else np.asarray(X_test)
        y_pred = self.model.predict(X_np)
        y_pred_proba = self.model.predict_proba(X_np)
        
        accuracy = accuracy_score(y_test, y_pred)
        
        print(f"\næ¨¡å‹è©•ä¼°çµæœ - {self.stock_id}")
        print("="*50)
        print(f"æº–ç¢ºç‡ (Accuracy): {accuracy:.4f}")
        print("\nåˆ†é¡å ±å‘Š:")
        print(classification_report(y_test, y_pred, target_names=['è³£å‡º/è§€æœ›', 'æŒæœ‰', 'è²·å…¥']))
        print("\næ··æ·†çŸ©é™£:")
        print(confusion_matrix(y_test, y_pred))
        
        # æ·»åŠ æ±ºç­–è§£é‡‹ç¯„ä¾‹
        if enable_explanation and len(X_test) > 0:
            print("\n" + "="*70)
            print("ğŸ“Š æ±ºç­–è§£é‡‹ç¯„ä¾‹ï¼ˆæœ€è¿‘3ç­†äº¤æ˜“ä¿¡è™Ÿï¼‰")
            print("="*70)
            
            # æ‰¾å‡ºè²·å…¥å’Œè³£å‡ºä¿¡è™Ÿçš„ç¯„ä¾‹
            buy_signals = np.where(y_pred == 2)[0]
            sell_signals = np.where(y_pred == 0)[0]
            hold_signals = np.where(y_pred == 1)[0]
            
            examples = []
            if len(buy_signals) > 0:
                examples.append(('è²·å…¥', buy_signals[-1]))
            if len(sell_signals) > 0:
                examples.append(('è³£å‡º', sell_signals[-1]))
            if len(hold_signals) > 0:
                examples.append(('æŒæœ‰', hold_signals[-1]))
            
            for signal_type, idx in examples[:3]:
                print(f"\nâ–¶ {signal_type}ä¿¡è™Ÿ - æ—¥æœŸ: {X_test.index[idx].date() if hasattr(X_test, 'index') else f'ç¬¬{idx}ç­†'}")
                
                # å–å¾—è©²ç­†è³‡æ–™çš„è§£é‡‹
                X_sample = X_test.iloc[[idx]] if isinstance(X_test, pd.DataFrame) else X_np[idx:idx+1]
                explanation = self.model.explain_prediction(X_sample, self.feature_cols)
                
                print(f"  é æ¸¬: {explanation['prediction_label']}")
                print(f"  ä¿¡å¿ƒåº¦: è²·å…¥={explanation['probabilities']['è²·å…¥']:.1%}, "
                      f"æŒæœ‰={explanation['probabilities']['æŒæœ‰']:.1%}, "
                      f"è³£å‡º={explanation['probabilities']['è³£å‡º/è§€æœ›']:.1%}")
                
                print(f"\n  é—œéµå½±éŸ¿å› ç´ ï¼ˆå‰5åï¼‰ï¼š")
                for i, feat in enumerate(explanation['top_features'][:5], 1):
                    print(f"    {i}. {feat['name']:<20} = {feat['value']:>8.4f}  "
                          f"(æ•æ„Ÿåº¦: {feat['sensitivity']:>+8.4f}) - {feat['impact']}")
            
            print("="*70)
        
        return y_pred, y_pred_proba, accuracy
    
    def save_model(self, filepath):
        """å„²å­˜æ¨¡å‹ï¼ˆæ”¯æ´ Prototypical Network æŒä¹…åŒ–ï¼‰"""
        script_dir = os.path.dirname(os.path.abspath(__file__))
        full_path = os.path.join(script_dir, filepath)

        # è‹¥ç‚º ProtoNetï¼Œä¿å­˜å…¶ stateï¼›å¦å‰‡å˜—è©¦ç›´æ¥ä¿å­˜ï¼ˆç›¸å®¹èˆŠç‰ˆï¼‰
        if isinstance(self.model, ProtoNet):
            model_state = self.model.get_state()
            model_data = {
                'model_type': 'prototypical',
                'model_state': model_state,
                'feature_cols': self.feature_cols,
                'stock_id': self.stock_id
            }
        else:
            model_data = {
                'model_type': 'legacy',
                'model': self.model,
                'feature_cols': self.feature_cols,
                'stock_id': self.stock_id
            }
        joblib.dump(model_data, full_path)
        print(f"\næ¨¡å‹å·²å„²å­˜è‡³: {full_path}")
    
    @staticmethod
    def load_model(filepath):
        """è¼‰å…¥æ¨¡å‹ï¼ˆæ”¯æ´ ProtoNetï¼‰"""
        if not os.path.isabs(filepath):
            script_dir = os.path.dirname(os.path.abspath(__file__))
            filepath = os.path.join(script_dir, filepath)

        model_data = joblib.load(filepath)
        stock_model = StockTradingModel(
            stock_id=model_data['stock_id'],
            feature_cols=model_data['feature_cols']
        )

        if model_data.get('model_type') == 'prototypical':
            state = model_data['model_state']
            stock_model.model = ProtoNet.from_state(state)
        else:
            stock_model.model = model_data.get('model')

        print(f"æ¨¡å‹å·²è¼‰å…¥: {filepath}")
        return stock_model


# ==================== ç¬¬äº”éƒ¨åˆ†ï¼šå›æ¸¬ç³»çµ± ====================

class Backtester:
    """å›æ¸¬ç³»çµ±"""
    
    def __init__(self, initial_capital=1000000, commission_rate=0.002):
        """
        Args:
            initial_capital: åˆå§‹è³‡é‡‘ (é è¨­100è¬)
            commission_rate: å–®é‚Šäº¤æ˜“æˆæœ¬ (é è¨­0.2%)
        """
        self.initial_capital = initial_capital
        self.commission_rate = commission_rate
    
    def backtest_portfolio(self, stocks_data, stocks_predictions, allocation_strategy='equal'):
        """
        åŸ·è¡Œå¤šè‚¡ç¥¨æŠ•è³‡çµ„åˆå›æ¸¬
        
        Args:
            stocks_data: dict, {stock_id: test_data}
            stocks_predictions: dict, {stock_id: predictions}
            allocation_strategy: è³‡é‡‘é…ç½®ç­–ç•¥ ('equal'=å¹³å‡åˆ†é…, 'dynamic'=å‹•æ…‹é…ç½®)
        
        Returns:
            dict: å›æ¸¬çµæœçµ±è¨ˆ
        """
        # ç¢ºä¿æ‰€æœ‰è‚¡ç¥¨çš„æ—¥æœŸç´¢å¼•ä¸€è‡´
        all_dates = None
        for stock_id, data in stocks_data.items():
            if all_dates is None:
                all_dates = data.index
            else:
                all_dates = all_dates.intersection(data.index)
        
        # åˆå§‹åŒ–
        capital = self.initial_capital
        positions = {stock_id: 0 for stock_id in stocks_data.keys()}  # å„è‚¡æŒå€‰
        trades = []  # äº¤æ˜“è¨˜éŒ„
        portfolio_values = []  # æŠ•è³‡çµ„åˆåƒ¹å€¼
        
        for i in range(len(all_dates) - 1):
            current_date = all_dates[i]
            
            # è¨ˆç®—ç•¶å‰æŠ•è³‡çµ„åˆåƒ¹å€¼
            current_value = capital
            for stock_id, position in positions.items():
                if position > 0:
                    current_price = stocks_data[stock_id].loc[current_date, 'Close']
                    current_value += position * current_price * (1 - self.commission_rate)
            
            portfolio_values.append({
                'Date': current_date,
                'Value': current_value
            })
            
            # ç²å–ä¸‹ä¸€å€‹äº¤æ˜“æ—¥çš„è³‡è¨Š
            next_date = all_dates[i + 1]
            
            # æ”¶é›†æ‰€æœ‰è‚¡ç¥¨çš„è¨Šè™Ÿ
            signals = {}
            for stock_id in stocks_data.keys():
                pred_idx = list(stocks_data[stock_id].index).index(current_date)
                prediction = stocks_predictions[stock_id][pred_idx]
                next_open = stocks_data[stock_id].loc[next_date, 'Open']
                signals[stock_id] = {
                    'prediction': prediction,
                    'next_open': next_open
                }
            
            # æ±ºå®šäº¤æ˜“ç­–ç•¥
            if allocation_strategy == 'equal':
                # å¹³å‡åˆ†é…ç­–ç•¥ï¼šå°‡è³‡é‡‘å¹³å‡åˆ†é…çµ¦æ‰€æœ‰è²·å…¥è¨Šè™Ÿçš„è‚¡ç¥¨
                buy_signals = [sid for sid, sig in signals.items() if sig['prediction'] == 2 and positions[sid] == 0]
                sell_signals = [sid for sid, sig in signals.items() if sig['prediction'] == 0 and positions[sid] > 0]
                
                # å…ˆè³£å‡º
                for stock_id in sell_signals:
                    if positions[stock_id] > 0:
                        next_open = signals[stock_id]['next_open']
                        revenue = positions[stock_id] * next_open * (1 - self.commission_rate)
                        capital += revenue
                        trades.append({
                            'Date': next_date,
                            'Stock': stock_id,
                            'Type': 'SELL',
                            'Price': next_open,
                            'Shares': positions[stock_id],
                            'Capital': capital
                        })
                        positions[stock_id] = 0
                
                # å†è²·å…¥
                if buy_signals:
                    # å°‡å¯ç”¨è³‡é‡‘å¹³å‡åˆ†é…
                    capital_per_stock = capital / len(buy_signals)
                    for stock_id in buy_signals:
                        next_open = signals[stock_id]['next_open']
                        shares = int(capital_per_stock / next_open)
                        if shares > 0:
                            cost = shares * next_open * (1 + self.commission_rate)
                            if cost <= capital:
                                capital -= cost
                                positions[stock_id] = shares
                                trades.append({
                                    'Date': next_date,
                                    'Stock': stock_id,
                                    'Type': 'BUY',
                                    'Price': next_open,
                                    'Shares': shares,
                                    'Capital': capital
                                })
        
        # æœ€å¾Œæ¸…ç®—æ‰€æœ‰æŒå€‰
        final_date = all_dates[-1]
        for stock_id, position in positions.items():
            if position > 0:
                final_close = stocks_data[stock_id].loc[final_date, 'Close']
                capital += position * final_close * (1 - self.commission_rate)
                trades.append({
                    'Date': final_date,
                    'Stock': stock_id,
                    'Type': 'SELL',
                    'Price': final_close,
                    'Shares': position,
                    'Capital': capital
                })
        
        # è¨ˆç®—ç¸¾æ•ˆæŒ‡æ¨™
        final_value = capital
        total_return = (final_value - self.initial_capital) / self.initial_capital
        
        # è¨ˆç®—æœ€å¤§å›æ’¤
        portfolio_df = pd.DataFrame(portfolio_values)
        portfolio_df['Peak'] = portfolio_df['Value'].cummax()
        portfolio_df['Drawdown'] = (portfolio_df['Value'] - portfolio_df['Peak']) / portfolio_df['Peak']
        max_drawdown = portfolio_df['Drawdown'].min()
        
        # è¨ˆç®—å¹´åŒ–å ±é…¬
        days = (all_dates[-1] - all_dates[0]).days
        years = days / 365.25
        annual_return = (1 + total_return) ** (1 / years) - 1 if years > 0 else 0
        
        results = {
            'initial_capital': self.initial_capital,
            'final_value': final_value,
            'total_return': total_return,
            'annual_return': annual_return,
            'max_drawdown': max_drawdown,
            'num_trades': len(trades),
            'trades': trades,
            'portfolio_values': portfolio_df
        }
        
        return results
    
    def print_detailed_trades(self, trades, stocks_names=None):
        """
        è©³ç´°å°å‡ºæ‰€æœ‰äº¤æ˜“è¨˜éŒ„ï¼ŒåŒ…æ‹¬äº¤æ˜“å¾Œçš„æŒå€‰ç‹€æ…‹
        
        Args:
            trades: äº¤æ˜“è¨˜éŒ„åˆ—è¡¨
            stocks_names: è‚¡ç¥¨åç¨±å­—å…¸ {stock_id: stock_name}
        """
        if not trades:
            print("\næ²’æœ‰äº¤æ˜“è¨˜éŒ„")
            return
        
        print("\n" + "="*120)
        print("è©³ç´°äº¤æ˜“è¨˜éŒ„èˆ‡æŒå€‰ç‹€æ…‹")
        print("="*120)
        print(f"{'åºè™Ÿ':<5} {'æ—¥æœŸ':<12} {'è‚¡ç¥¨':<18} {'å‹•ä½œ':<8} {'åƒ¹æ ¼':<10} {'è‚¡æ•¸':<8} {'äº¤æ˜“é‡‘é¡':<13} {'å‰©é¤˜ç¾é‡‘':<13} {'ç•¶å‰æŒå€‰':<30}")
        print("-"*120)
        
        # è¿½è¹¤ç•¶å‰æŒå€‰ç‹€æ…‹
        current_positions = {}
        
        for idx, trade in enumerate(trades, 1):
            date_str = trade['Date'].strftime('%Y-%m-%d')
            stock_id = trade['Stock']
            stock_name = stocks_names.get(stock_id, stock_id) if stocks_names else stock_id
            trade_type = trade['Type']
            price = trade['Price']
            shares = trade['Shares']
            capital = trade['Capital']
            
            # è¨ˆç®—äº¤æ˜“é‡‘é¡
            if trade_type == 'BUY':
                amount = shares * price * (1 + self.commission_rate)
                action_symbol = 'è²·å…¥ â†‘'
                current_positions[stock_id] = current_positions.get(stock_id, 0) + shares
            else:
                amount = shares * price * (1 - self.commission_rate)
                action_symbol = 'è³£å‡º â†“'
                current_positions[stock_id] = current_positions.get(stock_id, 0) - shares
                if current_positions[stock_id] == 0:
                    del current_positions[stock_id]
            
            # é¡¯ç¤ºè‚¡ç¥¨åç¨±
            display_name = f"{stock_name[:6]}({stock_id[:7]})"
            
            # é¡¯ç¤ºç•¶å‰æŒå€‰
            if current_positions:
                positions_str = ", ".join([f"{sid[:7]}:{shares}è‚¡" for sid, shares in current_positions.items()])
            else:
                positions_str = "ç©ºå€‰"
            
            print(f"{idx:<5} {date_str:<12} {display_name:<18} {action_symbol:<8} ${price:>8.2f} {shares:>7} ${amount:>11,.0f} ${capital:>11,.0f} {positions_str:<30}")
        
        print("="*120)
        print(f"ç¸½è¨ˆ {len(trades)} ç­†äº¤æ˜“")
        print("="*120)
        
    def backtest(self, test_data, predictions):
        """
        åŸ·è¡Œå›æ¸¬
        
        Args:
            test_data: æ¸¬è©¦é›†è³‡æ–™
            predictions: æ¨¡å‹é æ¸¬çµæœ
        
        Returns:
            dict: å›æ¸¬çµæœçµ±è¨ˆ
        """
        df = test_data.copy()
        df['Prediction'] = predictions
        
        # åˆå§‹åŒ–
        capital = self.initial_capital
        position = 0  # æŒå€‰æ•¸é‡
        trades = []  # äº¤æ˜“è¨˜éŒ„
        portfolio_values = []  # æŠ•è³‡çµ„åˆåƒ¹å€¼
        
        for i in range(len(df) - 1):
            current_date = df.index[i]
            next_open = df.iloc[i + 1]['Open']
            current_prediction = df.iloc[i]['Prediction']
            
            # æ ¹æ“šé æ¸¬è¨Šè™Ÿæ±ºå®šäº¤æ˜“ï¼ˆ0=è³£å‡º, 1=æŒæœ‰, 2=è²·å…¥ï¼‰
            if current_prediction == 2 and position == 0:
                # è²·å…¥è¨Šè™Ÿï¼šç”¨æ‰€æœ‰è³‡é‡‘è²·å…¥
                shares = int(capital / next_open)
                if shares > 0:
                    cost = shares * next_open * (1 + self.commission_rate)
                    capital -= cost
                    position = shares
                    trades.append({
                        'Date': df.index[i + 1],
                        'Type': 'BUY',
                        'Price': next_open,
                        'Shares': shares,
                        'Capital': capital
                    })
            
            elif current_prediction == 0 and position > 0:
                # è³£å‡ºè¨Šè™Ÿï¼šæ¸…å€‰
                revenue = position * next_open * (1 - self.commission_rate)
                capital += revenue
                trades.append({
                    'Date': df.index[i + 1],
                    'Type': 'SELL',
                    'Price': next_open,
                    'Shares': position,
                    'Capital': capital
                })
                position = 0
            
            # ç•¶ current_prediction == 1 æ™‚ï¼Œä¿æŒæŒæœ‰ç‹€æ…‹ï¼ˆä¸åšä»»ä½•å‹•ä½œï¼‰
            
            # è¨ˆç®—ç•¶å‰æŠ•è³‡çµ„åˆåƒ¹å€¼
            current_value = capital
            if position > 0:
                current_value += position * df.iloc[i]['Close'] * (1 - self.commission_rate)
            
            portfolio_values.append({
                'Date': current_date,
                'Value': current_value
            })
        
        # æœ€å¾Œå¦‚æœé‚„æœ‰æŒå€‰ï¼ŒæŒ‰æœ€å¾Œä¸€å¤©æ”¶ç›¤åƒ¹è³£å‡º
        if position > 0:
            final_close = df.iloc[-1]['Close']
            capital += position * final_close * (1 - self.commission_rate)
            trades.append({
                'Date': df.index[-1],
                'Type': 'SELL',
                'Price': final_close,
                'Shares': position,
                'Capital': capital
            })
        
        # è¨ˆç®—ç¸¾æ•ˆæŒ‡æ¨™
        final_value = capital
        total_return = (final_value - self.initial_capital) / self.initial_capital
        
        # è¨ˆç®—æœ€å¤§å›æ’¤
        portfolio_df = pd.DataFrame(portfolio_values)
        portfolio_df['Peak'] = portfolio_df['Value'].cummax()
        portfolio_df['Drawdown'] = (portfolio_df['Value'] - portfolio_df['Peak']) / portfolio_df['Peak']
        max_drawdown = portfolio_df['Drawdown'].min()
        
        # è¨ˆç®—å¹´åŒ–å ±é…¬
        days = (df.index[-1] - df.index[0]).days
        years = days / 365.25
        annual_return = (1 + total_return) ** (1 / years) - 1 if years > 0 else 0
        
        results = {
            'initial_capital': self.initial_capital,
            'final_value': final_value,
            'total_return': total_return,
            'annual_return': annual_return,
            'max_drawdown': max_drawdown,
            'num_trades': len(trades),
            'trades': trades,
            'portfolio_values': portfolio_df
        }
        
        return results
    
    def calculate_buy_and_hold(self, test_data):
        """è¨ˆç®—è²·å…¥æŒæœ‰ç­–ç•¥çš„ç¸¾æ•ˆ"""
        first_price = test_data.iloc[0]['Open']
        last_price = test_data.iloc[-1]['Close']
        
        shares = int(self.initial_capital / first_price)
        buy_cost = shares * first_price * (1 + self.commission_rate)
        sell_revenue = shares * last_price * (1 - self.commission_rate)
        
        final_value = sell_revenue + (self.initial_capital - buy_cost)
        total_return = (final_value - self.initial_capital) / self.initial_capital
        
        days = (test_data.index[-1] - test_data.index[0]).days
        years = days / 365.25
        annual_return = (1 + total_return) ** (1 / years) - 1 if years > 0 else 0
        
        return {
            'final_value': final_value,
            'total_return': total_return,
            'annual_return': annual_return
        }
    
    def print_results(self, results, benchmark_results=None):
        """å°å‡ºå›æ¸¬çµæœ"""
        print("\n" + "="*70)
        print("å›æ¸¬çµæœæ‘˜è¦")
        print("="*70)
        print(f"åˆå§‹è³‡é‡‘ï¼š${results['initial_capital']:,.0f}")
        print(f"æœ€çµ‚è³‡é‡‘ï¼š${results['final_value']:,.0f}")
        print(f"ç¸½å ±é…¬ç‡ï¼š{results['total_return']:.2%}")
        print(f"å¹´åŒ–å ±é…¬ï¼š{results['annual_return']:.2%}")
        print(f"æœ€å¤§å›æ’¤ï¼š{results['max_drawdown']:.2%}")
        print(f"äº¤æ˜“æ¬¡æ•¸ï¼š{results['num_trades']}")
        
        if benchmark_results:
            print("\n" + "-"*70)
            print("åŸºæº–æ¯”è¼ƒ (Buy-and-Hold)")
            print("-"*70)
            print(f"åŸºæº–ç¸½å ±é…¬ç‡ï¼š{benchmark_results['total_return']:.2%}")
            print(f"åŸºæº–å¹´åŒ–å ±é…¬ï¼š{benchmark_results['annual_return']:.2%}")
            print(f"è¶…é¡å ±é…¬ï¼š{(results['total_return'] - benchmark_results['total_return']):.2%}")
        
        print("="*70)


# ==================== ç¬¬å…­éƒ¨åˆ†ï¼šæ¯æ—¥äº’å‹•æ¨è«– ====================

def daily_inference(stock_id, model_path, enable_explanation=True):
    """
    æ¯æ—¥æ”¶ç›¤å¾ŒåŸ·è¡Œçš„æ¨è«–è…³æœ¬ï¼ˆå¢å¼·ç‰ˆï¼šå«æ±ºç­–è§£é‡‹ï¼‰
    
    Args:
        stock_id: è‚¡ç¥¨ä»£ç¢¼
        model_path: æ¨¡å‹è·¯å¾‘
        enable_explanation: æ˜¯å¦è¼¸å‡ºæ±ºç­–è§£é‡‹
    
    Returns:
        dict: æ¨è«–çµæœ
    """
    print(f"\n{'='*70}")
    print(f"ğŸ“ˆ æ¯æ—¥æ¨è«– - {stock_id}")
    print(f"åŸ·è¡Œæ™‚é–“ï¼š{datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print(f"{'='*70}")
    
    # 1. è¼‰å…¥æ¨¡å‹
    model = StockTradingModel.load_model(model_path)
    
    # 2. ä¸‹è¼‰æœ€æ–°è³‡æ–™ (éœ€è¦è¶³å¤ çš„æ­·å²è³‡æ–™ä»¥è¨ˆç®—æŠ€è¡“æŒ‡æ¨™)
    end_date = datetime.now()
    start_date = end_date - timedelta(days=120)  # æŠ“å–è¿‘4å€‹æœˆè³‡æ–™
    
    data = yf.download(stock_id, start=start_date.strftime('%Y-%m-%d'), 
                      end=end_date.strftime('%Y-%m-%d'), progress=False)
    
    if len(data) == 0:
        print("âŒ éŒ¯èª¤ï¼šç„¡æ³•ç²å–è³‡æ–™")
        return None
    
    # 3. å»ºç«‹ç‰¹å¾µ
    df = engineer_features(data)
    df = df.dropna()
    
    if len(df) == 0:
        print("âŒ éŒ¯èª¤ï¼šç‰¹å¾µè¨ˆç®—å¾Œç„¡æœ‰æ•ˆè³‡æ–™")
        return None
    
    # 4. å–æœ€æ–°ä¸€ç­†è³‡æ–™é€²è¡Œé æ¸¬
    latest_features = df[model.feature_cols].iloc[[-1]]
    latest_date = df.index[-1]
    latest_close = df.iloc[-1]['Close']
    
    # 5. æ¨¡å‹æ¨è«–ï¼ˆè½‰ç‚º numpy ä»¥ç¢ºä¿ç›¸å®¹ï¼‰
    prediction = model.model.predict(latest_features.values)[0]
    probabilities = model.model.predict_proba(latest_features.values)[0]
    
    # 6. è¼¸å‡ºçµæœ
    action_map = {0: 'è³£å‡º/è§€æœ›', 1: 'æŒæœ‰', 2: 'è²·å…¥'}
    action_emoji = {0: 'ğŸ”»', 1: 'â¸ï¸', 2: 'ğŸš€'}
    
    print(f"\nğŸ“… æœ€æ–°è³‡æ–™æ—¥æœŸï¼š{latest_date.date()}")
    print(f"ğŸ’° æ”¶ç›¤åƒ¹ï¼š${latest_close:.2f}")
    print(f"\n{action_emoji[prediction]} é æ¸¬çµæœï¼š{action_map[prediction]} (å»ºè­°æ“ä½œ)")
    print(f"   è²·å…¥æ©Ÿç‡ï¼š{probabilities[2]:>6.1%} {'â–ˆ' * int(probabilities[2]*20)}")
    print(f"   æŒæœ‰æ©Ÿç‡ï¼š{probabilities[1]:>6.1%} {'â–ˆ' * int(probabilities[1]*20)}")
    print(f"   è³£å‡ºæ©Ÿç‡ï¼š{probabilities[0]:>6.1%} {'â–ˆ' * int(probabilities[0]*20)}")
    
    # 7. æ±ºç­–è§£é‡‹
    explanation = None
    if enable_explanation:
        print(f"\n{'â”€'*70}")
        print("ğŸ” æ±ºç­–åˆ†æï¼šç‚ºä»€éº¼æ¨¡å‹åšå‡ºé€™å€‹é æ¸¬ï¼Ÿ")
        print(f"{'â”€'*70}")
        
        explanation = model.model.explain_prediction(latest_features, model.feature_cols)
        
        print(f"\nğŸ“Š åˆ°å„é¡åˆ¥åŸå‹çš„è·é›¢ï¼š")
        for dist_name, dist_val in explanation['distances'].items():
            print(f"   {dist_name}: {dist_val:.4f}")
        
        print(f"\nâ­ é—œéµå½±éŸ¿å› ç´ ï¼ˆå‰5åï¼‰ï¼š")
        for i, feat in enumerate(explanation['top_features'], 1):
            impact_symbol = 'â†‘' if feat['sensitivity'] > 0 else 'â†“'
            print(f"   {i}. {feat['name']:<20} = {feat['value']:>8.4f}")
            print(f"      æ•æ„Ÿåº¦: {feat['sensitivity']:>+8.4f} {impact_symbol} {feat['impact']}")
    
    # 8. é¡¯ç¤ºé—œéµæŠ€è¡“æŒ‡æ¨™
    print(f"\n{'â”€'*70}")
    print("ğŸ“ˆ æŠ€è¡“æŒ‡æ¨™ç¸½è¦½ï¼š")
    print(f"{'â”€'*70}")
    print(f"   RSI (ç›¸å°å¼·å¼±)ï¼š{df.iloc[-1]['RSI']*100:.2f} "
          f"{'ğŸ”¥è¶…è²·' if df.iloc[-1]['RSI'] > 0.7 else 'â„ï¸è¶…è³£' if df.iloc[-1]['RSI'] < 0.3 else 'âœ…ä¸­æ€§'}")
    print(f"   SMAä¹–é›¢ç‡ï¼š{df.iloc[-1]['SMA_Deviation']:.2%} "
          f"{'â¬†ï¸åé«˜' if df.iloc[-1]['SMA_Deviation'] > 0.05 else 'â¬‡ï¸åä½' if df.iloc[-1]['SMA_Deviation'] < -0.05 else 'â¡ï¸æ­£å¸¸'}")
    print(f"   ATRæ¨™æº–åŒ–ï¼ˆæ³¢å‹•ï¼‰ï¼š{df.iloc[-1]['ATR_Normalized']:.4f}")
    print(f"   5æ—¥å°æ•¸æ”¶ç›Šï¼š{df.iloc[-1]['LogReturn_5']:.4f}")
    print(f"   10æ—¥å°æ•¸æ”¶ç›Šï¼š{df.iloc[-1]['LogReturn_10']:.4f}")
    print(f"   20æ—¥å°æ•¸æ”¶ç›Šï¼š{df.iloc[-1]['LogReturn_20']:.4f}")
    
    signal_map = {0: 'SELL', 1: 'HOLD', 2: 'BUY'}
    result = {
        'stock_id': stock_id,
        'date': latest_date,
        'close_price': latest_close,
        'prediction': prediction,
        'probabilities': probabilities,
        'signal': signal_map[prediction],
        'explanation': explanation if enable_explanation else None
    }
    
    print(f"\n{'='*70}\n")
    
    return result


# ==================== ç¬¬ä¸ƒéƒ¨åˆ†ï¼šä¸»ç¨‹å¼ ====================

def main():
    """ä¸»ç¨‹å¼ï¼šåŸ·è¡Œå®Œæ•´çš„è¨“ç·´èˆ‡å›æ¸¬æµç¨‹"""
    
    print("="*80)
    print("ğŸš€ AI é‡‘èäº¤æ˜“ç³»çµ± - å°è‚¡ MVP å°ˆæ¡ˆ (æŠ•è³‡çµ„åˆæ¨¡å¼ + æ±ºç­–è§£é‡‹)")
    print("="*80)
    
    # é¡¯ç¤ºç•¶å‰é…ç½®
    config = TradingConfig()
    print("\nğŸ“Š ç•¶å‰ç³»çµ±é…ç½®ï¼š")
    print("-"*80)
    print(f"æ¨¡å‹æ¶æ§‹ï¼šEmbedding={config.EMBEDDING_DIM}, Hidden={config.HIDDEN_DIMS}, Dropout={config.DROPOUT}")
    print(f"è¨“ç·´è¨­å®šï¼šLR={config.LEARNING_RATE}, Epochs={config.EPOCHS}, Episodes={config.EPISODES_PER_EPOCH}")
    print(f"äº¤æ˜“ç­–ç•¥ï¼šè²·å…¥é–€æª»={config.BUY_THRESHOLD:.2%}, è³£å‡ºé–€æª»={config.SELL_THRESHOLD:.2%}")
    print(f"Few-Shotï¼šK={config.K_SHOT}, Q={config.Q_QUERY}, Temperature={config.TEMPERATURE}")
    print("-"*80)
    print("ğŸ’¡ å¦‚éœ€èª¿æ•´åƒæ•¸ä»¥å„ªåŒ–æ¨¡å‹ï¼Œè«‹åƒé–± 'å¯¦é©—èª¿æ•´æŒ‡å—.md'\n")
    
    # å®šç¾©æ¨™çš„è‚¡ç¥¨
    stocks = {
        '2330.TW': 'å°ç©é›» (æ¬Šå€¼å‹)',
        '2317.TW': 'é´»æµ· (è¶¨å‹¢å‹)',
        '2603.TW': 'é•·æ¦® (é«˜æ³¢å‹•å‹)'
    }
    
    benchmark = '0050.TW'  # åŸºæº–æŒ‡æ•¸
    
    # å„²å­˜æ‰€æœ‰æ¨¡å‹å’Œé æ¸¬çµæœ
    all_models = {}
    all_test_data = {}
    all_predictions = {}
    all_accuracies = {}
    
    # å–å¾—è…³æœ¬æ‰€åœ¨ç›®éŒ„
    script_dir = os.path.dirname(os.path.abspath(__file__))
    
    # ç¬¬ä¸€éšæ®µï¼šè¨“ç·´/è¼‰å…¥æ‰€æœ‰è‚¡ç¥¨çš„æ¨¡å‹
    print("\n" + "="*70)
    print("ç¬¬ä¸€éšæ®µï¼šè¨“ç·´/è¼‰å…¥æ¨¡å‹")
    print("="*70)
    
    for stock_id, stock_name in stocks.items():
        print(f"\n\n{'#'*70}")
        print(f"è™•ç†è‚¡ç¥¨ï¼š{stock_name} ({stock_id})")
        print(f"{'#'*70}")
        
        try:
            # å®šç¾©æ¨¡å‹æª”æ¡ˆè·¯å¾‘
            model_filename = f'model_{stock_id.replace(".TW", "")}.pkl'
            model_path = os.path.join(script_dir, model_filename)
            
            # æª¢æŸ¥æ¨¡å‹æ˜¯å¦å·²å­˜åœ¨
            if os.path.exists(model_path):
                print(f"\nåµæ¸¬åˆ°å·²å­˜åœ¨çš„æ¨¡å‹æª”æ¡ˆ: {model_path}")
                print("æ­£åœ¨è¼‰å…¥æ¨¡å‹...")
                model = StockTradingModel.load_model(model_filename)
                
                # ä¸‹è¼‰è³‡æ–™ç”¨æ–¼å›æ¸¬
                data = download_stock_data(stock_id)
                df = engineer_features(data)
                df['Target'] = create_labels(df, threshold=0.004, hold_threshold=0.002)
                df = df.dropna()
                test_data = df.loc['2024-01-01':]
                
                # ä½¿ç”¨è¼‰å…¥çš„æ¨¡å‹é€²è¡Œé æ¸¬
                X_test = test_data[model.feature_cols]
                y_test = test_data['Target']
                y_pred, y_pred_proba, accuracy = model.evaluate(X_test, y_test)
                
            else:
                print(f"\næœªæ‰¾åˆ°æ¨¡å‹æª”æ¡ˆï¼Œé–‹å§‹è¨“ç·´æ–°æ¨¡å‹...")
                
                # 1. ä¸‹è¼‰è³‡æ–™
                data = download_stock_data(stock_id)
                
                # 2. åˆå§‹åŒ–æ¨¡å‹
                model = StockTradingModel(stock_id)
                
                # 3. æº–å‚™è³‡æ–™
                X_train, X_test, y_train, y_test, train_data, test_data = model.prepare_data(data)
                
                # 4. è¨“ç·´æ¨¡å‹
                model.train(X_train, y_train)
                
                # 5. è©•ä¼°æ¨¡å‹
                y_pred, y_pred_proba, accuracy = model.evaluate(X_test, y_test)
                
                # 6. å„²å­˜æ¨¡å‹
                model.save_model(model_filename)
            
            # å„²å­˜çµæœ
            all_models[stock_id] = model
            all_test_data[stock_id] = test_data
            all_predictions[stock_id] = y_pred
            all_accuracies[stock_id] = accuracy
            
        except Exception as e:
            print(f"éŒ¯èª¤ï¼šè™•ç† {stock_id} æ™‚ç™¼ç”Ÿç•°å¸¸ï¼š{str(e)}")
            continue
    
    # ç¬¬äºŒéšæ®µï¼šä½¿ç”¨æŠ•è³‡çµ„åˆç­–ç•¥å›æ¸¬ï¼ˆå…±ç”¨100è¬è³‡é‡‘ï¼‰
    print("\n\n" + "="*70)
    print("ç¬¬äºŒéšæ®µï¼šæŠ•è³‡çµ„åˆå›æ¸¬ï¼ˆä¸‰æª”è‚¡ç¥¨å…±ç”¨100è¬è³‡é‡‘ï¼‰")
    print("="*70)
    
    if len(all_test_data) == len(stocks):
        backtester = Backtester(initial_capital=1000000)
        portfolio_results = backtester.backtest_portfolio(all_test_data, all_predictions)
        
        print("\næŠ•è³‡çµ„åˆå›æ¸¬çµæœï¼š")
        print("="*70)
        print(f"åˆå§‹è³‡é‡‘ï¼š${portfolio_results['initial_capital']:,.0f}")
        print(f"æœ€çµ‚è³‡é‡‘ï¼š${portfolio_results['final_value']:,.0f}")
        print(f"ç¸½å ±é…¬ç‡ï¼š{portfolio_results['total_return']:.2%}")
        print(f"å¹´åŒ–å ±é…¬ï¼š{portfolio_results['annual_return']:.2%}")
        print(f"æœ€å¤§å›æ’¤ï¼š{portfolio_results['max_drawdown']:.2%}")
        print(f"ç¸½äº¤æ˜“æ¬¡æ•¸ï¼š{portfolio_results['num_trades']}")
        
        # çµ±è¨ˆå„è‚¡ç¥¨çš„äº¤æ˜“æ¬¡æ•¸
        trade_counts = {}
        for trade in portfolio_results['trades']:
            stock = trade['Stock']
            trade_counts[stock] = trade_counts.get(stock, 0) + 1
        
        print("\nå„è‚¡ç¥¨äº¤æ˜“æ¬¡æ•¸ï¼š")
        for stock_id, count in trade_counts.items():
            stock_name = stocks[stock_id]
            print(f"  {stock_name} ({stock_id}): {count} æ¬¡")
        
        print("="*70)
        
        # é¡¯ç¤ºè©³ç´°äº¤æ˜“è¨˜éŒ„
        if portfolio_results['num_trades'] > 0:
            backtester.print_detailed_trades(portfolio_results['trades'], stocks)
    else:
        print("\nè­¦å‘Šï¼šéƒ¨åˆ†è‚¡ç¥¨æ¨¡å‹æœªèƒ½æˆåŠŸè¼‰å…¥ï¼Œç„¡æ³•åŸ·è¡ŒæŠ•è³‡çµ„åˆå›æ¸¬")
        portfolio_results = None
    
    # ç¬¬ä¸‰éšæ®µï¼šä¸‹è¼‰ä¸¦æ¯”è¼ƒ 0050
    print(f"\n\n{'#'*70}")
    print(f"æ¯”è¼ƒåŸºæº–æŒ‡æ•¸ï¼š0050.TW")
    print(f"{'#'*70}")
    
    try:
        benchmark_data = download_stock_data(benchmark)
        test_benchmark = benchmark_data.loc['2024-01-01':]
        
        backtester_bench = Backtester(initial_capital=1000000)
        benchmark_0050 = backtester_bench.calculate_buy_and_hold(test_benchmark)
        
        print("\n0050 è²·å…¥æŒæœ‰ç­–ç•¥ç¸¾æ•ˆï¼š")
        total_return = float(benchmark_0050['total_return'])
        annual_return = float(benchmark_0050['annual_return'])
        print(f"ç¸½å ±é…¬ç‡ï¼š{total_return:.2%}")
        print(f"å¹´åŒ–å ±é…¬ï¼š{annual_return:.2%}")
        
    except Exception as e:
        print(f"éŒ¯èª¤ï¼šè™•ç† 0050 æ™‚ç™¼ç”Ÿç•°å¸¸ï¼š{str(e)}")
        benchmark_0050 = None
    
    # æœ€çµ‚ç¸½çµ
    print("\n\n" + "="*70)
    print("æœ€çµ‚ç¸¾æ•ˆç¸½çµ")
    print("="*70)
    
    print("\nã€å„è‚¡ç¥¨æ¨¡å‹æº–ç¢ºç‡ã€‘")
    print("-"*70)
    for stock_id, accuracy in all_accuracies.items():
        stock_name = stocks[stock_id]
        print(f"{stock_name:<20} ({stock_id}): {accuracy:.2%}")
    
    if portfolio_results:
        print("\nã€æŠ•è³‡çµ„åˆç­–ç•¥ vs åŸºæº–æŒ‡æ•¸ã€‘")
        print("-"*70)
        print(f"{'ç­–ç•¥':<30} {'ç¸½å ±é…¬ç‡':<15} {'å¹´åŒ–å ±é…¬':<15} {'æœ€å¤§å›æ’¤':<15}")
        print("-"*70)
        print(f"{'AIæŠ•è³‡çµ„åˆ (ä¸‰æª”å…±100è¬)':<30} {portfolio_results['total_return']:>12.2%}  {portfolio_results['annual_return']:>12.2%}  {portfolio_results['max_drawdown']:>12.2%}")
        
        if benchmark_0050:
            print(f"{'0050è²·å…¥æŒæœ‰ (100è¬)':<30} {benchmark_0050['total_return']:>12.2%}  {benchmark_0050['annual_return']:>12.2%}  {'N/A':>12}")
            excess = portfolio_results['total_return'] - benchmark_0050['total_return']
            print("-"*70)
            print(f"{'è¶…é¡å ±é…¬':<30} {excess:>12.2%}")
    
    print("="*70)
    
    print("\nå°ˆæ¡ˆåŸ·è¡Œå®Œæˆï¼")
    print("æŠ•è³‡çµ„åˆæ¨¡å¼ï¼šä½¿ç”¨100è¬è³‡é‡‘åŒæ™‚æ“ä½œä¸‰æª”è‚¡ç¥¨")
    print("æ‚¨å¯ä»¥ä½¿ç”¨ daily_inference() å‡½æ•¸é€²è¡Œæ¯æ—¥æ¨è«–")
    print("ç¯„ä¾‹ï¼šdaily_inference('2330.TW', 'model_2330.pkl')")
    
    return {
        'portfolio_results': portfolio_results,
        'accuracies': all_accuracies,
        'benchmark_0050': benchmark_0050
    }


if __name__ == "__main__":
    # åŸ·è¡Œä¸»ç¨‹å¼
    results = main()
    
    # å¯é¸ï¼šåŸ·è¡Œæ¯æ—¥æ¨è«–ç¯„ä¾‹
    # daily_inference('2330.TW', 'model_2330.pkl')
